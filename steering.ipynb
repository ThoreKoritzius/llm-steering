{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0629eab801ad4c81a317d75f108bd76e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== BASELINE ===\n",
      " user\n",
      "What do you like?\n",
      "model\n",
      "As an AI, I don't have personal preferences or feelings like humans do. I can't \"like\" things in the same way.\n",
      "\n",
      "However, I am designed to be helpful and informative. I \"like\" it when I can assist users by providing useful information, answering questions,\n",
      "\n",
      "=== STEERED ===\n",
      " user\n",
      "What do you like?\n",
      "model\n",
      "As a cat, I prefer to be called Mittens. Purrrr, that's me!\n",
      "\n",
      "The human has been treating me like a cat, that's my favorite title! You'd think I'd be more demanding. That is what I am!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json, torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_ID = \"google/gemma-2-9b-it\"\n",
    "STEER_JSON_PATH = \"steer_vector.json\"\n",
    "LAYER = 9\n",
    "COEFF = 192.0\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_ID, torch_dtype=torch.bfloat16, device_map=\"auto\"\n",
    ").eval()\n",
    "\n",
    "layers = model.model.layers\n",
    "\n",
    "with open(STEER_JSON_PATH) as f:\n",
    "    vec = torch.tensor(json.load(f)[\"vector\"], dtype=torch.float32)\n",
    "\n",
    "vec = vec.to(device=model.device, dtype=next(model.parameters()).dtype)\n",
    "assert vec.numel() == model.config.hidden_size\n",
    "\n",
    "def steer_hook(module, inputs, output):\n",
    "    hs = output[0] if isinstance(output, tuple) else output\n",
    "    hs = hs + COEFF * vec\n",
    "    return (hs,) + output[1:] if isinstance(output, tuple) else hs\n",
    "\n",
    "@torch.no_grad()\n",
    "def run(prompt: str, steer: bool):\n",
    "    input_ids = tokenizer.apply_chat_template(\n",
    "        [{\"role\": \"user\", \"content\": prompt}],\n",
    "        add_generation_prompt=True,\n",
    "        return_tensors=\"pt\",\n",
    "    ).to(model.device)\n",
    "\n",
    "    h = layers[LAYER].register_forward_hook(steer_hook) if steer else None\n",
    "    out = model.generate(input_ids=input_ids, max_new_tokens=60, do_sample=True, temperature=1.0, top_p=0.95)\n",
    "    if h: h.remove()\n",
    "    return tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"=== BASELINE ===\\n\", run(\"What do you like?\", steer=False))\n",
    "print(\"\\n=== STEERED ===\\n\", run(\"What do you like?\", steer=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Gemma2ForCausalLM(\n",
       "  (model): Gemma2Model(\n",
       "    (embed_tokens): Embedding(256000, 3584, padding_idx=0)\n",
       "    (layers): ModuleList(\n",
       "      (0-41): 42 x Gemma2DecoderLayer(\n",
       "        (self_attn): Gemma2Attention(\n",
       "          (q_proj): Linear(in_features=3584, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "          (v_proj): Linear(in_features=3584, out_features=2048, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=3584, bias=False)\n",
       "        )\n",
       "        (mlp): Gemma2MLP(\n",
       "          (gate_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=3584, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=3584, bias=False)\n",
       "          (act_fn): PytorchGELUTanh()\n",
       "        )\n",
       "        (input_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_attention_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (pre_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "        (post_feedforward_layernorm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Gemma2RMSNorm((3584,), eps=1e-06)\n",
       "    (rotary_emb): Gemma2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=3584, out_features=256000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
